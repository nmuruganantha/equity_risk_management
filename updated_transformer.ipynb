{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import io # Keep for robustness, though direct file read is now primary\n",
    "\n",
    "# --- 1. Global Constants (Adjust as needed) ---\n",
    "LOOK_BACK = 1300      # Number of past time steps the model looks at\n",
    "PREDICT_AHEAD = 10    # Number of future time steps the model predicts\n",
    "EMBED_DIM = 128       # Embedding dimension for the Transformer\n",
    "NUM_HEADS = 8         # Number of attention heads in MultiHeadSelfAttention\n",
    "FF_DIM = 256          # Hidden layer size in feed forward network of TransformerBlock\n",
    "DROPOUT_RATE = 0.2    # Dropout rate for regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Data Loading and Preprocessing (UPDATED) ---\n",
    "def load_and_preprocess_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file path, separates input and target features, and scales them.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - numpy.ndarray: The scaled input features data (X).\n",
    "            - numpy.ndarray: The scaled target feature data (Y).\n",
    "            - sklearn.preprocessing.MinMaxScaler: Scaler fitted on input features.\n",
    "            - sklearn.preprocessing.MinMaxScaler: Scaler fitted on target feature.\n",
    "            - list: List of input feature names.\n",
    "            - list: List of target feature names.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {csv_file_path}...\")\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    # Define input features (used by the model to predict)\n",
    "    # Removed 'Closing_Price' from input features as it's now the target\n",
    "    input_features = ['volume', 'RSI', 'MACD', 'MACD_Signal', 'Momentum', 'OBV', 'ATR', 'Revenue_Growth', 'EPS_Growth', 'ROE']\n",
    "\n",
    "    # Define the single target feature to be predicted\n",
    "    target_feature = ['Closing_Price'] # Predicting 'Closing_Price'\n",
    "\n",
    "    # Select and preprocess input features\n",
    "    df_input_features = df[input_features]\n",
    "    df_input_features = df_input_features.fillna(method='ffill').fillna(method='bfill')\n",
    "    print(f\"Input DataFrame shape: {df_input_features.shape}\")\n",
    "\n",
    "    # Select and preprocess target feature\n",
    "    df_target_feature = df[target_feature]\n",
    "    df_target_feature = df_target_feature.fillna(method='ffill').fillna(method='bfill')\n",
    "    print(f\"Target DataFrame shape: {df_target_feature.shape}\")\n",
    "\n",
    "    # Initialize separate scalers for input and target features\n",
    "    scaler_input = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_output = MinMaxScaler(feature_range=(0, 1)) # This scaler is for the target\n",
    "\n",
    "    # Fit and transform input features\n",
    "    scaled_input_data = scaler_input.fit_transform(df_input_features)\n",
    "\n",
    "    # Fit and transform target feature\n",
    "    scaled_target_data = scaler_output.fit_transform(df_target_feature)\n",
    "\n",
    "    return scaled_input_data, scaled_target_data, scaler_input, scaler_output, input_features, target_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Sequence Creation (UPDATED) ---\n",
    "def create_sequences(input_data, target_data, look_back, predict_ahead):\n",
    "    \"\"\"\n",
    "    Creates input (X) and target (Y) sequences for time series prediction.\n",
    "\n",
    "    Args:\n",
    "        input_data (numpy.ndarray): The scaled input features data.\n",
    "        target_data (numpy.ndarray): The scaled target feature data.\n",
    "        look_back (int): Number of past time steps to use as input.\n",
    "        predict_ahead (int): Number of future time steps to predict.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - numpy.ndarray: Input sequences (X).\n",
    "            - numpy.ndarray: Target sequences (Y).\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    # Ensure we don't go out of bounds for both input and target data\n",
    "    # The latest possible starting point for a sequence is when \n",
    "    # (i + look_back + predict_ahead) is still within the bounds of the data.\n",
    "    for i in range(len(input_data) - look_back - predict_ahead + 1):\n",
    "        # X is the sequence of input features for 'look_back' steps\n",
    "        X.append(input_data[i:(i + look_back)])\n",
    "        # Y is the sequence of target feature (Closing_Price) for 'predict_ahead' steps\n",
    "        Y.append(target_data[(i + look_back):(i + look_back + predict_ahead)])\n",
    "    return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Transformer Model Components ---\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention mechanism as described in the Transformer paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=8, **kwargs):\n",
    "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    A single Transformer block combining Multi-Head Attention and a Feed-Forward Network.\n",
    "    (Fixed: added training=None to call signature)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None): # <-- FIX: Added training=None\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding layer to add temporal information to input embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs): # inputs here are already of shape (batch_size, sequence_length, embed_dim)\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embedding(positions)\n",
    "        # The addition works because inputs and embedded_positions now have compatible last dimensions (embed_dim)\n",
    "        return inputs * self.scale + embedded_positions\n",
    "\n",
    "\n",
    "def build_transformer_model(input_shape, output_sequence_length, output_features_count):\n",
    "    \"\"\"\n",
    "    Builds the Transformer model for time series prediction.\n",
    "    (Fixed: Added TimeDistributed Dense for input feature projection and Flatten before final Dense)\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input sequences (look_back, num_input_features).\n",
    "        output_sequence_length (int): The length of the output sequence (predict_ahead).\n",
    "        output_features_count (int): The number of features in the output (now 1 for Closing_Price).\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Transformer model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape) # (None, LOOK_BACK, num_input_features)\n",
    "\n",
    "    # Project input features (num_input_features) to EMBED_DIM before positional embedding\n",
    "    x = layers.TimeDistributed(layers.Dense(EMBED_DIM))(inputs) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
    "\n",
    "    # Positional Embedding for input sequence\n",
    "    x = PositionalEmbedding(input_shape[0], EMBED_DIM)(x) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
    "\n",
    "    # Apply Transformer blocks\n",
    "    transformer_block = TransformerBlock(EMBED_DIM, NUM_HEADS, FF_DIM, DROPOUT_RATE)\n",
    "    x = transformer_block(x) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
    "\n",
    "    # Flatten the output from the TransformerBlock so that the Dense layer can operate on a single vector per sample.\n",
    "    # This aggregates information from all LOOK_BACK timesteps into a single vector for prediction.\n",
    "    x = layers.Flatten()(x) # Shape: (None, LOOK_BACK * EMBED_DIM)\n",
    "\n",
    "    # The Dense layer now outputs a flat vector of `predict_ahead * output_features_count`\n",
    "    # output_features_count is now 1 for 'Closing_Price'\n",
    "    outputs = layers.Dense(output_sequence_length * output_features_count)(x) # Shape: (None, PREDICT_AHEAD * 1)\n",
    "\n",
    "    # Reshape to the desired output sequence shape: (batch_size, predict_ahead, output_features_count)\n",
    "    outputs = layers.Reshape((output_sequence_length, output_features_count))(outputs) # Shape: (None, PREDICT_AHEAD, 1)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Model Training ---\n",
    "def train_transformer_ts(X_scaled, Y_scaled, input_features_count, output_features_count, look_back, predict_ahead, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the Transformer time series prediction model.\n",
    "\n",
    "    Args:\n",
    "        X_scaled (numpy.ndarray): The scaled input sequences.\n",
    "        Y_scaled (numpy.ndarray): The scaled target sequences.\n",
    "        input_features_count (int): Number of features in the input data.\n",
    "        output_features_count (int): Number of features in the output (target) data (now 1).\n",
    "        look_back (int): The sequence length for inputs.\n",
    "        predict_ahead (int): The sequence length for outputs (predictions).\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The trained Transformer model.\n",
    "    \"\"\"\n",
    "    # Build the model with updated input and output feature counts\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(look_back, input_features_count), # Pass the count of input features\n",
    "        output_sequence_length=predict_ahead,\n",
    "        output_features_count=output_features_count # Pass the count of target features (should be 1)\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "\n",
    "    print(f\"Training Transformer model for {epochs} epochs...\")\n",
    "    model.fit(X_scaled, Y_scaled, epochs=epochs, batch_size=batch_size, verbose=0) # Set verbose to 1 for progress\n",
    "    print(\"Training complete.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Prediction Function (UPDATED) ---\n",
    "def predict_future_values(model, last_input_sequence_scaled, scaler_output):\n",
    "    \"\"\"\n",
    "    Predicts future values using the trained Transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): The trained Transformer model.\n",
    "        last_input_sequence_scaled (numpy.ndarray): The last sequence of input data, scaled.\n",
    "                                                    Shape: (look_back, num_input_features) before expansion.\n",
    "        scaler_output (sklearn.preprocessing.MinMaxScaler): The scaler used for the single target feature.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The inverse-transformed predicted future values.\n",
    "                       Shape: (predict_ahead, 1).\n",
    "    \"\"\"\n",
    "    # Ensure the input sequence has the correct shape for prediction (batch_size, look_back, num_input_features)\n",
    "    if last_input_sequence_scaled.ndim == 2:\n",
    "        last_input_sequence_scaled = np.expand_dims(last_input_sequence_scaled, axis=0)\n",
    "\n",
    "    # Make prediction\n",
    "    predicted_scaled = model.predict(last_input_sequence_scaled, verbose=0)\n",
    "    # After prediction, predicted_scaled will have shape (1, PREDICT_AHEAD, 1)\n",
    "\n",
    "    # Reshape the predicted output to 2D for inverse transformation: (PREDICT_AHEAD, 1)\n",
    "    # The scaler expects a 2D array where columns are features. Since we have 1 feature,\n",
    "    # we reshape from (1, PREDICT_AHEAD, 1) to (PREDICT_AHEAD, 1).\n",
    "    predicted_scaled_2d = predicted_scaled.reshape(predicted_scaled.shape[1], predicted_scaled.shape[2])\n",
    "\n",
    "    # Inverse transform the prediction\n",
    "    predicted_original = scaler_output.inverse_transform(predicted_scaled_2d)\n",
    "\n",
    "    # predicted_original is already (PREDICT_AHEAD, 1) after inverse_transform, no further reshape needed if it's the only target\n",
    "    # However, the line below is robust if shape[1] is PREDICT_AHEAD and shape[2] is 1.\n",
    "    # It's technically redundant if predicted_scaled_2d was already (PREDICT_AHEAD, 1)\n",
    "    # but doesn't hurt.\n",
    "    # If you want to explicitly ensure (PREDICT_AHEAD, 1) without relying on reshape's behavior:\n",
    "    # predicted_original = predicted_original.flatten().reshape(-1, 1)\n",
    "    # Or simply: return predicted_original if it's already (PREDICT_AHEAD, 1)\n",
    "\n",
    "    return predicted_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage (UPDATED) ---\n",
    "\n",
    "csv_file = \"data/AAPL_market_data.csv\"\n",
    "\n",
    "# 1. Load and preprocess data (now returns separate input and target data/scalers)\n",
    "scaled_input_data, scaled_target_data, scaler_input, scaler_output, input_features_list, target_features_list = load_and_preprocess_data(csv_file)\n",
    "\n",
    "# Determine the number of features based on the preprocessed data\n",
    "NUM_INPUT_FEATURES = scaled_input_data.shape[1]  # Number of features in X\n",
    "NUM_TARGET_FEATURES = scaled_target_data.shape[1] # Number of features in Y (should be 1 for 'Closing_Price')\n",
    "\n",
    "# 2. Create sequences (now takes both scaled_input_data and scaled_target_data)\n",
    "X, Y = create_sequences(scaled_input_data, scaled_target_data, LOOK_BACK, PREDICT_AHEAD)\n",
    "\n",
    "# Ensure X and Y are reshaped correctly for the model\n",
    "# X: (samples, look_back, NUM_INPUT_FEATURES)\n",
    "# Y: (samples, predict_ahead, NUM_TARGET_FEATURES)\n",
    "\n",
    "# 3. Train the model\n",
    "if X.shape[0] > 0: # Check if sequences were created\n",
    "    model = train_transformer_ts(\n",
    "        X_scaled=X,\n",
    "        Y_scaled=Y,\n",
    "        input_features_count=NUM_INPUT_FEATURES,  # Pass input feature count\n",
    "        output_features_count=NUM_TARGET_FEATURES, # Pass target feature count (1)\n",
    "        look_back=LOOK_BACK,\n",
    "        predict_ahead=PREDICT_AHEAD,\n",
    "        epochs=1,  # Set a small number of epochs for demonstration\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    # 4. Make a prediction\n",
    "    # Get the last sequence from the scaled input data to predict future values\n",
    "    last_input_sequence_scaled = scaled_input_data[-LOOK_BACK:]\n",
    "\n",
    "    if last_input_sequence_scaled.shape[0] == LOOK_BACK:\n",
    "        predicted_future_values = predict_future_values(model, last_input_sequence_scaled, scaler_output)\n",
    "        print(f\"\\nPredicted Future Values (original scale) for next {PREDICT_AHEAD} steps for '{target_features_list[0]}':\")\n",
    "        \n",
    "        # Get the last known date from the original DataFrame for date generation\n",
    "        original_df_full = pd.read_csv(csv_file)\n",
    "        original_df_full['date'] = pd.to_datetime(original_df_full['date'])\n",
    "        last_known_date = original_df_full['date'].iloc[-1]\n",
    "\n",
    "        # Generate future dates (business days)\n",
    "        future_dates = pd.date_range(start=last_known_date + pd.Timedelta(days=1),\n",
    "                                     periods=PREDICT_AHEAD,\n",
    "                                     freq='B') # 'B' for business day frequency\n",
    "\n",
    "        # Create a DataFrame for better readability of predictions with dates\n",
    "        predicted_df = pd.DataFrame(predicted_future_values, columns=target_features_list)\n",
    "        predicted_df.insert(0, 'date', future_dates)\n",
    "        print(predicted_df)\n",
    "    else:\n",
    "        print(\"Not enough data to create the last input sequence for prediction.\")\n",
    "else:\n",
    "    print(\"Not enough data to create sequences for training and prediction. Adjust LOOK_BACK or PREDICT_AHEAD constants or provide more data.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
