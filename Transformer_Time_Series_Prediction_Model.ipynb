{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers # type: ignore\n",
        "from datetime import datetime, timedelta\n",
        "import io\n",
        "import warnings\n",
        "\n",
        "# Suppress FutureWarning from scikit-learn\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_dataframe_to_csv(df: pd.DataFrame, ticker: str, filename: str = None) -> bool:\n",
        "    \"\"\"\n",
        "    Saves a pandas DataFrame to a CSV file.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
        "        print(\"Error: Input is not a valid or non-empty DataFrame. Nothing to save.\")\n",
        "        return False\n",
        "\n",
        "    if filename is None:\n",
        "        filename = f\"{ticker}_market_data.csv\"\n",
        "\n",
        "    try:\n",
        "        # Save the DataFrame to a CSV file.\n",
        "        # The index (which is the date) is crucial, so we ensure it's saved.\n",
        "        df.to_csv(filename, index=True)\n",
        "        print(f\"\\nDataFrame successfully saved to '{filename}'\")\n",
        "        return True\n",
        "    except IOError as e:\n",
        "        # Handle specific file system errors\n",
        "        print(f\"\\nAn I/O error occurred while saving the file: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Global Constants\n",
        "\n",
        "LOOK_BACK = 130       # Number of past time steps the model looks at (approx 6 months of trading days)\n",
        "PREDICT_AHEAD = 10    # Number of future time steps the model predicts\n",
        "EMBED_DIM = 256       # Embedding dimension for the Transformer (must be divisible by NUM_HEADS)\n",
        "NUM_HEADS = 16        # Number of attention heads (EMBED_DIM / NUM_HEADS = 256 / 16 = 16, which is valid)\n",
        "FF_DIM = 1024         # Hidden layer size in the feed-forward network\n",
        "DROPOUT_RATE = 0.2    # Dropout rate for regularization\n",
        "NUM_TRANSFORMER_BLOCKS = 4 # Number of Transformer Blocks to stack\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Data Loading and Preprocessing\n",
        "def load_and_preprocess_data(csv_file_path):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV file path, separates input and target features, and scales them.\n",
        "\n",
        "    Args:\n",
        "        csv_file_path (str): The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - numpy.ndarray: The scaled input features data (X).\n",
        "            - numpy.ndarray: The scaled target feature data (Y).\n",
        "            - sklearn.preprocessing.MinMaxScaler: Scaler fitted on input features.\n",
        "            - sklearn.preprocessing.MinMaxScaler: Scaler fitted on target feature.\n",
        "            - list: List of input feature names.\n",
        "            - list: List of target feature names.\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {csv_file_path}...\")\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.set_index('date')\n",
        "\n",
        "    # Define input features (used by the model to predict)\n",
        "    input_features = ['RSI', 'MACD', 'MACD_Signal', 'Momentum', 'OBV', 'ATR', 'Revenue_Growth', 'EPS_Growth', 'ROE']\n",
        "\n",
        "    # Define the single target feature to be predicted\n",
        "    target_feature = ['Daily_Return'] # Predicting 'Daily_Return'\n",
        "\n",
        "    # Select and preprocess input features\n",
        "    df_input_features = df[input_features]\n",
        "    df_input_features = df_input_features.fillna(method='ffill').fillna(method='bfill')\n",
        "    print(f\"Input DataFrame shape: {df_input_features.shape}\")\n",
        "\n",
        "    # Select and preprocess target feature\n",
        "    df_target_feature = df[target_feature]\n",
        "    df_target_feature = df_target_feature.fillna(method='ffill').fillna(method='bfill')\n",
        "    print(f\"Target DataFrame shape: {df_target_feature.shape}\")\n",
        "\n",
        "    # Initialize separate scalers for input and target features\n",
        "    scaler_input = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaler_output = MinMaxScaler(feature_range=(0, 1)) # This scaler is for the target\n",
        "\n",
        "    # Fit and transform input features\n",
        "    scaled_input_data = scaler_input.fit_transform(df_input_features)\n",
        "\n",
        "    # Fit and transform target feature\n",
        "    scaled_target_data = scaler_output.fit_transform(df_target_feature)\n",
        "\n",
        "    return scaled_input_data, scaled_target_data, scaler_input, scaler_output, input_features, target_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Sequence Creation\n",
        "def create_sequences(input_data, target_data, look_back, predict_ahead):\n",
        "    \"\"\"\n",
        "    Creates input (X) and target (Y) sequences for time series prediction.\n",
        "\n",
        "    Args:\n",
        "        input_data (numpy.ndarray): The scaled input features data.\n",
        "        target_data (numpy.ndarray): The scaled target feature data.\n",
        "        look_back (int): Number of past time steps to use as input.\n",
        "        predict_ahead (int): Number of future time steps to predict.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - numpy.ndarray: Input sequences (X).\n",
        "            - numpy.ndarray: Target sequences (Y).\n",
        "    \"\"\"\n",
        "    X, Y = [], []\n",
        "    # Ensure we don't go out of bounds for both input and target data\n",
        "    # The latest possible starting point for a sequence is when \n",
        "    # (i + look_back + predict_ahead) is still within the bounds of the data.\n",
        "    for i in range(len(input_data) - look_back - predict_ahead + 1):\n",
        "        # X is the sequence of input features for 'look_back' steps\n",
        "        X.append(input_data[i:(i + look_back)])\n",
        "        # Y is the sequence of target feature (Daily_Return) for 'predict_ahead' steps\n",
        "        Y.append(target_data[(i + look_back):(i + look_back + predict_ahead)])\n",
        "    return np.array(X), np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  4.1 Transformer Model Architecture\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention layer as described in the Transformer paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads=8, **kwargs):\n",
        "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  4.2 Transformer Model Architecture\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    \"\"\"\n",
        "    A single Transformer block combining Multi-Head Attention and a Feed-Forward Network.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=None): # <--- FIX IS HERE: Add `training=None`\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  4.3 Transformer Model Architecture\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    Positional Embedding layer to inject sequence order information.\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence_length, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.embed_dim = embed_dim\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embedding(positions)\n",
        "        return inputs * self.scale + embedded_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Model Training\n",
        "def build_transformer_model(input_shape, output_sequence_length, output_features_count, num_transformer_blocks=1):\n",
        "    \"\"\"\n",
        "    Builds the Transformer model for time series prediction with multiple Transformer blocks.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input sequences (look_back, num_input_features).\n",
        "        output_sequence_length (int): The length of the output sequence (predict_ahead).\n",
        "        output_features_count (int): The number of features in the output (now 1 for 'Daily_Return').\n",
        "        num_transformer_blocks (int): The number of TransformerBlock layers to stack.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: The compiled Transformer model.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape) # (None, LOOK_BACK, num_input_features)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.Dense(EMBED_DIM))(inputs) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
        "\n",
        "    x = PositionalEmbedding(input_shape[0], EMBED_DIM)(x) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
        "\n",
        "    # Stack multiple Transformer blocks\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        transformer_block = TransformerBlock(EMBED_DIM, NUM_HEADS, FF_DIM, DROPOUT_RATE)\n",
        "        x = transformer_block(x) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
        "\n",
        "    x = layers.Flatten()(x) # Shape: (None, LOOK_BACK * EMBED_DIM)\n",
        "\n",
        "    outputs = layers.Dense(output_sequence_length * output_features_count)(x) # Shape: (None, PREDICT_AHEAD * 1)\n",
        "\n",
        "    outputs = layers.Reshape((output_sequence_length, output_features_count))(outputs) # Shape: (None, PREDICT_AHEAD, 1)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Training Function ---\n",
        "def train_transformer_ts(X_scaled, Y_scaled, input_features_count, output_features_count, look_back, predict_ahead, epochs=50, batch_size=32, num_transformer_blocks=1):\n",
        "    \"\"\"\n",
        "    Trains the Transformer time series prediction model.\n",
        "\n",
        "    Args:\n",
        "        X_scaled (numpy.ndarray): The scaled input sequences.\n",
        "        Y_scaled (numpy.ndarray): The scaled target sequences.\n",
        "        input_features_count (int): Number of features in the input data.\n",
        "        output_features_count (int): Number of features in the output (target) data (now 1).\n",
        "        look_back (int): The sequence length for inputs.\n",
        "        predict_ahead (int): The sequence length for outputs (predictions).\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size for training.\n",
        "        num_transformer_blocks (int): Number of Transformer blocks to use in the model.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: The trained Transformer model.\n",
        "    \"\"\"\n",
        "    model = build_transformer_model(\n",
        "        input_shape=(look_back, input_features_count),\n",
        "        output_sequence_length=predict_ahead,\n",
        "        output_features_count=output_features_count,\n",
        "        num_transformer_blocks=num_transformer_blocks\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "\n",
        "    print(f\"Training Transformer model for {epochs} epochs with {num_transformer_blocks} blocks...\")\n",
        "    # It's highly recommended to use a validation split to monitor for overfitting\n",
        "    model.fit(X_scaled, Y_scaled, epochs=epochs, batch_size=batch_size, verbose=1, validation_split=0.2)\n",
        "    print(\"Training complete.\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "eW51h5UNHQX2"
      },
      "outputs": [],
      "source": [
        "# --- 6. Prediction Function ---\n",
        "def predict_future_values(model, last_input_sequence_scaled, scaler_output):\n",
        "    \"\"\"\n",
        "    Predicts future values using the trained Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model (keras.Model): The trained Transformer model.\n",
        "        last_input_sequence_scaled (numpy.ndarray): The last sequence of input data, scaled.\n",
        "                                                    Shape: (look_back, num_input_features) before expansion.\n",
        "        scaler_output (sklearn.preprocessing.MinMaxScaler): The scaler used for the single target feature.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The inverse-transformed predicted future values.\n",
        "                       Shape: (predict_ahead, 1).\n",
        "    \"\"\"\n",
        "    # Ensure the input sequence has the correct shape for prediction (batch_size, look_back, num_input_features)\n",
        "    if last_input_sequence_scaled.ndim == 2:\n",
        "        last_input_sequence_scaled = np.expand_dims(last_input_sequence_scaled, axis=0)\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_scaled = model.predict(last_input_sequence_scaled, verbose=0)\n",
        "    # After prediction, predicted_scaled will have shape (1, PREDICT_AHEAD, 1)\n",
        "\n",
        "    # Reshape the predicted output to 2D for inverse transformation: (PREDICT_AHEAD, 1)\n",
        "    # The scaler expects a 2D array where columns are features. Since we have 1 feature,\n",
        "    # we reshape from (1, PREDICT_AHEAD, 1) to (PREDICT_AHEAD, 1).\n",
        "    predicted_scaled_2d = predicted_scaled.reshape(predicted_scaled.shape[1], predicted_scaled.shape[2])\n",
        "\n",
        "    # Inverse transform the prediction\n",
        "    predicted_original = scaler_output.inverse_transform(predicted_scaled_2d)\n",
        "\n",
        "    # predicted_original is already (PREDICT_AHEAD, 1) after inverse_transform, no further reshape needed if it's the only target\n",
        "    # However, the line below is robust if shape[1] is PREDICT_AHEAD and shape[2] is 1.\n",
        "    # It's technically redundant if predicted_scaled_2d was already (PREDICT_AHEAD, 1)\n",
        "    # but doesn't hurt.\n",
        "    # If you want to explicitly ensure (PREDICT_AHEAD, 1) without relying on reshape's behavior:\n",
        "    # predicted_original = predicted_original.flatten().reshape(-1, 1)\n",
        "    # Or simply: return predicted_original if it's already (PREDICT_AHEAD, 1)\n",
        "\n",
        "    return predicted_original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Prediction Function ---\n",
        "def predict_future_values(model, last_input_sequence_scaled, scaler_output):\n",
        "    \"\"\"\n",
        "    Predicts future daily returns using the trained Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model (keras.Model): The trained Transformer model.\n",
        "        last_input_sequence_scaled (numpy.ndarray): The last sequence of input data, scaled.\n",
        "                                                    Shape: (look_back, num_input_features) before expansion.\n",
        "        scaler_output (sklearn.preprocessing.MinMaxScaler): The scaler used for the 'Daily_Return' target feature.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The inverse-transformed predicted future daily returns.\n",
        "                       Shape: (predict_ahead, 1).\n",
        "    \"\"\"\n",
        "    if last_input_sequence_scaled.ndim == 2:\n",
        "        last_input_sequence_scaled = np.expand_dims(last_input_sequence_scaled, axis=0)\n",
        "\n",
        "    predicted_scaled_returns = model.predict(last_input_sequence_scaled, verbose=0)\n",
        "    \n",
        "    predicted_scaled_returns_2d = predicted_scaled_returns.reshape(predicted_scaled_returns.shape[1], predicted_scaled_returns.shape[2])\n",
        "\n",
        "    predicted_original_returns = scaler_output.inverse_transform(predicted_scaled_returns_2d)\n",
        "    \n",
        "    return predicted_original_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconstruct_prices_dataframe(last_known_close_price, future_dates, predicted_daily_returns):\n",
        "    \"\"\"\n",
        "    Reconstructs future close prices from predicted daily returns and creates a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        last_known_close_price (float): The actual closing price of the last day in the input sequence.\n",
        "        future_dates (pd.DatetimeIndex or list of datetime): The dates for the future predictions.\n",
        "        predicted_daily_returns (numpy.ndarray): The predicted daily returns (unscaled).\n",
        "                                                  Shape: (PREDICT_AHEAD, 1).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with 'date', 'predicted daily return', and 'predicted price'.\n",
        "    \"\"\"\n",
        "    future_close_prices = []\n",
        "    current_price = last_known_close_price\n",
        "    \n",
        "    # Ensure predicted_daily_returns is flat for iteration\n",
        "    predicted_daily_returns_flat = predicted_daily_returns.flatten()\n",
        "\n",
        "    for i in range(len(predicted_daily_returns_flat)):\n",
        "        daily_return = predicted_daily_returns_flat[i] if not np.isnan(predicted_daily_returns_flat[i]) else 0.0\n",
        "        current_price = current_price * (1 + daily_return)\n",
        "        future_close_prices.append(current_price)\n",
        "    \n",
        "    # Create the DataFrame\n",
        "    reconstructed_df = pd.DataFrame({\n",
        "        'date': future_dates,\n",
        "        'predicted daily return': predicted_daily_returns_flat,\n",
        "        'predicted price': future_close_prices\n",
        "    })\n",
        "    \n",
        "    return reconstructed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'datetime' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[132], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      3\u001b[0m     symbol_to_process \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     fixed_start_date \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m(\u001b[38;5;241m2010\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m27\u001b[39m)\n\u001b[1;32m      5\u001b[0m     fixed_end_date \u001b[38;5;241m=\u001b[39m datetime(\u001b[38;5;241m2024\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m31\u001b[39m)\n\u001b[1;32m      6\u001b[0m     fmp_api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOUR_FMP_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# <<<<<<< REMEMBER TO REPLACE THIS\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ],
      "source": [
        "# --- Example Usage ---\n",
        "if __name__ == '__main__':\n",
        "    symbol_to_process = \"AAPL\"\n",
        "    fixed_start_date = datetime(2010, 4, 27)\n",
        "    fixed_end_date = datetime(2024, 12, 31)\n",
        "    fmp_api_key = \"YOUR_FMP_API_KEY\" # <<<<<<< REMEMBER TO REPLACE THIS\n",
        "\n",
        "    # Step 1: Prepare the DataFrame (fetches data and calculates indicators - returns UNCALED data)\n",
        "    prepared_df = prepare_dataframe(symbol_to_process, fixed_start_date, fixed_end_date, fmp_api_key)\n",
        "\n",
        "    if prepared_df is not None and not prepared_df.empty:\n",
        "        print(\"\\n--- First few rows of Prepared DataFrame (UNSCALED) ---\")\n",
        "        print(prepared_df.head())\n",
        "        print(\"\\n--- Last few rows of Prepared DataFrame (UNSCALED) ---\")\n",
        "        print(prepared_df.tail())\n",
        "        print(f\"\\nPrepared DataFrame shape: {prepared_df.shape}\")\n",
        "        print(f\"Date range of the final prepared_df: {prepared_df.index.min()} to {prepared_df.index.max()}\")\n",
        "\n",
        "        # Step 2: Normalize the prepared data\n",
        "        scaled_input_data, scaled_target_data, scaler_input, scaler_output = normalize_prepared_data(\n",
        "            prepared_df, INPUT_FEATURES, OUTPUT_TARGETS\n",
        "        )\n",
        "\n",
        "        if scaled_input_data is not None and scaled_input_data.size > 0:\n",
        "            # Step 3: Create training and testing datasets (uses already scaled data)\n",
        "            X_sequences, Y_sequences = create_training_testing_datasets(\n",
        "                scaled_input_data, scaled_target_data, LOOK_BACK, PREDICT_AHEAD\n",
        "            )\n",
        "\n",
        "            if X_sequences is not None and X_sequences.size > 0:\n",
        "                NUM_INPUT_FEATURES = X_sequences.shape[2]\n",
        "                NUM_TARGET_FEATURES = Y_sequences.shape[2]\n",
        "\n",
        "                # Step 4: Train the model\n",
        "                model = train_transformer_ts(\n",
        "                    X_scaled=X_sequences,\n",
        "                    Y_scaled=Y_sequences,\n",
        "                    input_features_count=NUM_INPUT_FEATURES,\n",
        "                    output_features_count=NUM_TARGET_FEATURES,\n",
        "                    look_back=LOOK_BACK,\n",
        "                    predict_ahead=PREDICT_AHEAD,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS\n",
        "                )\n",
        "\n",
        "                # Step 5: Make a prediction\n",
        "                last_input_raw = prepared_df[INPUT_FEATURES].iloc[-LOOK_BACK:]\n",
        "\n",
        "                if last_input_raw.shape[0] == LOOK_BACK:\n",
        "                    last_input_sequence_scaled = scaler_input.transform(last_input_raw)\n",
        "                    \n",
        "                    # Get the last known close price from the prepared_df for reconstruction\n",
        "                    last_known_close_price = prepared_df['close'].iloc[-1]\n",
        "\n",
        "                    # Predict original daily returns\n",
        "                    predicted_original_returns = predict_future_values(\n",
        "                        model, last_input_sequence_scaled, scaler_output\n",
        "                    )\n",
        "                    \n",
        "                    last_known_date = prepared_df.index[-1]\n",
        "\n",
        "                    future_dates = pd.date_range(start=last_known_date + pd.Timedelta(days=1),\n",
        "                                                 periods=PREDICT_AHEAD,\n",
        "                                                 freq='B')\n",
        "\n",
        "                    # NEW: Use the reconstruct_prices_dataframe function\n",
        "                    predicted_df = reconstruct_prices_dataframe(\n",
        "                        last_known_close_price, future_dates, predicted_original_returns\n",
        "                    )\n",
        "                    \n",
        "                    print(f\"\\nPredicted Future Values (Daily Returns and Reconstructed Prices) for next {PREDICT_AHEAD} steps:\")\n",
        "                    print(predicted_df)\n",
        "                else:\n",
        "                    print(f\"Not enough data in prepared_df ({last_input_raw.shape[0]} rows) to create the last input sequence for prediction (requires {LOOK_BACK} rows).\")\n",
        "            else:\n",
        "                print(\"Not enough data to create sequences for training and prediction. Adjust LOOK_BACK or PREDICT_AHEAD constants or provide more data.\")\n",
        "        else:\n",
        "            print(\"Normalization failed or resulted in empty scaled data. Cannot proceed with sequence creation.\")\n",
        "    else:\n",
        "        print(\"Prepared DataFrame is empty or None. Cannot proceed with model training and prediction.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deeplearning-python-3.10_numpy1.26.4-M4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
