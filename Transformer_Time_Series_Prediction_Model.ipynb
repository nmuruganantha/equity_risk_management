{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers # type: ignore\n",
        "import io\n",
        "from typing import List, Tuple\n",
        "import warnings\n",
        "\n",
        "# Suppress FutureWarning from scikit-learn\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_testing_datasets(df, lookback_window=30, predict_horizon=1):\n",
        "    \"\"\"\n",
        "    Creates training and testing datasets (X, Y) from a prepared DataFrame,\n",
        "    applying scaling to both inputs and outputs.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(\"Error: Input DataFrame is empty or None.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    print(f\"\\n--- Creating Training/Testing Datasets ---\")\n",
        "\n",
        "    # Select features (inputs) and targets (outputs)\n",
        "    input_features = [\n",
        "        'RSI', 'MACD', 'MACD_Signal', 'Momentum', 'OBV', 'ATR',\n",
        "        'EPS_Growth', 'Revenue_Growth', 'ROE'\n",
        "    ]\n",
        "    output_targets = ['RSI', 'MACD', 'close'] # 'close' here refers to the future close price\n",
        "\n",
        "    # Ensure all required columns exist after calculations and merges\n",
        "    missing_cols = [col for col in input_features + output_targets if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Error: Missing required columns in DataFrame: {missing_cols}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Normalize indicators\n",
        "    scaler_input = MinMaxScaler()\n",
        "    scaler_output = MinMaxScaler()\n",
        "\n",
        "    # Create input and output sequences\n",
        "    X, Y = [], []\n",
        "    # The range for `i` must ensure that `i + lookback_window + predict_horizon - 1`\n",
        "    # does not exceed the length of the DataFrame.\n",
        "    # The last valid index for `Y` will be `len(df) - 1`.\n",
        "    # So, `i + lookback_window + predict_horizon - 1 <= len(df) - 1`\n",
        "    # which simplifies to `i <= len(df) - lookback_window - predict_horizon`.\n",
        "    for i in range(len(df) - lookback_window - predict_horizon + 1):\n",
        "        # Input: `lookback_window` days of selected indicators\n",
        "        X.append(df[input_features].iloc[i : i + lookback_window].values)\n",
        "        # Output: `predict_horizon` day's RSI, MACD, and 'close' price\n",
        "        Y.append(df[output_targets].iloc[i + lookback_window + predict_horizon - 1].values)\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    if X.size == 0 or Y.size == 0:\n",
        "        print(f\"Not enough data after windowing with lookback_window={lookback_window} and predict_horizon={predict_horizon}.\")\n",
        "        print(\"Adjust lookback_window or predict_horizon, or ensure sufficient input data.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Apply scaling to input features\n",
        "    # Reshape X to (n_samples * lookback_window, n_features) for scaling\n",
        "    original_shape_X = X.shape\n",
        "    X_reshaped_for_scaling = X.reshape(-1, original_shape_X[-1])\n",
        "    X_scaled_reshaped = scaler_input.fit_transform(X_reshaped_for_scaling)\n",
        "    X_scaled = X_scaled_reshaped.reshape(original_shape_X)\n",
        "\n",
        "    # Apply scaling to output targets\n",
        "    Y_scaled = scaler_output.fit_transform(Y)\n",
        "\n",
        "    print(f\"Created X_scaled with shape: {X_scaled.shape}\")\n",
        "    print(f\"Created Y_scaled with shape: {Y_scaled.shape}\")\n",
        "\n",
        "    return X_scaled, Y_scaled, scaler_input, scaler_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOOK_BACK: This constant defines the size of the input sequence (or \"look-back window\").\n",
        "# It specifies how many historical time steps the Transformer model will consider\n",
        "# when making a prediction.\n",
        "LOOK_BACK = 1300  # 5 year data\n",
        "# PREDICT_AHEAD: This constant determines the number of future time steps the model\n",
        "# will predict.\n",
        "PREDICT_AHEAD = 30\n",
        "# EMBED_DIM: This is the embedding dimension used throughout the Transformer model.\n",
        "# It represents the size of the vector space into which input features are projected.\n",
        "# A larger embedding dimension allows the model to capture more complex relationships\n",
        "# but also increases computational cost. It's crucial that this is divisible by NUM_HEADS.\n",
        "EMBED_DIM = 128\n",
        "# NUM_HEADS: This specifies the number of \"attention heads\" in the MultiHeadSelfAttention\n",
        "# layer. Multi-head attention allows the model to jointly attend to information from\n",
        "# different representation subspaces at different positions. More heads can capture\n",
        "# diverse patterns but also increase complexity.\n",
        "NUM_HEADS = 8\n",
        "# FF_DIM: This is the hidden dimension of the feed-forward network within each\n",
        "# TransformerBlock. The feed-forward network processes the output of the attention\n",
        "# mechanism. It typically expands the dimensionality before projecting it back\n",
        "# to the EMBED_DIM.\n",
        "FF_DIM = 256\n",
        "# DROPOUT_RATE: This is the dropout rate applied for regularization within the\n",
        "# TransformerBlocks. Dropout randomly sets a fraction of input units to 0 at each\n",
        "# update during training, which helps prevent overfitting by forcing the network\n",
        "# to learn more robust features.\n",
        "DROPOUT_RATE = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data Loading and Preprocessing ---\n",
        "def load_and_preprocess_data(csv_file_path): # Renamed argument to clarify it's a path\n",
        "    \"\"\"\n",
        "    Loads data from a CSV file path, selects relevant features, and scales them.\n",
        "    \"\"\"\n",
        "    # Load the CSV data into a pandas DataFrame\n",
        "    print(f\"Loading data from {csv_file_path}...\")\n",
        "    df = pd.read_csv(csv_file_path) # Now directly reads from the file path\n",
        "\n",
        "    # Convert 'date' column to datetime objects and set as index\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.set_index('date')\n",
        "\n",
        "    # Define the features to be used as input and output\n",
        "    # Based on the user's request: [RSI, MACD, MACD_Signal, Momentum, OBV, ATR, Revenue Growth, EPS Growth, ROE]\n",
        "    features = ['RSI', 'MACD', 'MACD_Signal', 'Momentum', 'OBV', 'ATR', 'Revenue_Growth', 'EPS_Growth', 'ROE']\n",
        "\n",
        "    # Select only the relevant features\n",
        "    df_features = df[features]\n",
        "    print(f\"Initial DataFrame shape: {df_features.shape}\")\n",
        "\n",
        "    # Handle missing values: forward fill then backfill to ensure no NaNs remain\n",
        "    df_features = df_features.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    # Initialize scalers for input and output features\n",
        "    # For time series prediction where input and output features are the same,\n",
        "    # we can use one scaler for simplicity, or separate if their scaling needs differ.\n",
        "    # Here, we'll use one scaler for the features themselves.\n",
        "    scaler_input = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaler_output = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    # Fit and transform the features\n",
        "    scaled_data = scaler_input.fit_transform(df_features)\n",
        "    # The output scaler will be fitted on the same data, but conceptually it's for inverse_transforming predictions\n",
        "    scaler_output.fit(df_features)\n",
        "\n",
        "    return scaled_data, scaler_input, scaler_output, features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_dataframe_to_csv(df: pd.DataFrame, ticker: str, filename: str = None) -> bool:\n",
        "    \"\"\"\n",
        "    Saves a pandas DataFrame to a CSV file.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
        "        print(\"Error: Input is not a valid or non-empty DataFrame. Nothing to save.\")\n",
        "        return False\n",
        "\n",
        "    if filename is None:\n",
        "        filename = f\"{ticker}_market_data.csv\"\n",
        "\n",
        "    try:\n",
        "        # Save the DataFrame to a CSV file.\n",
        "        # The index (which is the date) is crucial, so we ensure it's saved.\n",
        "        df.to_csv(filename, index=True)\n",
        "        print(f\"\\nDataFrame successfully saved to '{filename}'\")\n",
        "        return True\n",
        "    except IOError as e:\n",
        "        # Handle specific file system errors\n",
        "        print(f\"\\nAn I/O error occurred while saving the file: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        # Handle any other unexpected errors\n",
        "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences(data, look_back, predict_ahead):\n",
        "    \"\"\"\n",
        "    Creates input (X) and output (Y) sequences for the Transformer model.\n",
        "\n",
        "    Args:\n",
        "        data (numpy.ndarray): The scaled time series data.\n",
        "        look_back (int): The number of past time steps for each input sequence.\n",
        "        predict_ahead (int): The number of future time steps for each output sequence.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - numpy.ndarray: Input sequences (X).\n",
        "            - numpy.ndarray: Output sequences (Y).\n",
        "    \"\"\"\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - look_back - predict_ahead + 1):\n",
        "        # Input sequence: from current position 'i' up to 'look_back' steps\n",
        "        X.append(data[i:(i + look_back)])\n",
        "        # Output sequence: 'predict_ahead' steps starting right after the input sequence\n",
        "        Y.append(data[(i + look_back):(i + look_back + predict_ahead)])\n",
        "    return np.array(X), np.array(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Transformer Model Architecture ---\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention layer as described in the Transformer paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads=8, **kwargs):\n",
        "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    Positional Embedding layer to inject sequence order information.\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence_length, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.embed_dim = embed_dim\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embedding(positions)\n",
        "        return inputs * self.scale + embedded_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    \"\"\"\n",
        "    A single Transformer block combining Multi-Head Attention and a Feed-Forward Network.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=None): # <--- FIX IS HERE: Add `training=None`\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_transformer_model(input_shape, output_sequence_length, output_features_count):\n",
        "    \"\"\"\n",
        "    Builds the Transformer model for time series prediction.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input sequences (look_back, num_features).\n",
        "        output_sequence_length (int): The length of the output sequence (predict_ahead).\n",
        "        output_features_count (int): The number of features in the output.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: The compiled Transformer model.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape) # (None, LOOK_BACK, NUM_FEATURES)\n",
        "\n",
        "    # Project input features to EMBED_DIM before positional embedding\n",
        "    x = layers.TimeDistributed(layers.Dense(EMBED_DIM))(inputs) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
        "\n",
        "    # Positional Embedding for input sequence\n",
        "    x = PositionalEmbedding(input_shape[0], EMBED_DIM)(x) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
        "\n",
        "    # Apply Transformer blocks\n",
        "    transformer_block = TransformerBlock(EMBED_DIM, NUM_HEADS, FF_DIM, DROPOUT_RATE)\n",
        "    # Pass the `training` argument, which Keras automatically provides when\n",
        "    # defining a model via the functional API.\n",
        "    x = transformer_block(x, training=True) # <--- FIX: Pass `training=True` here, or better, pass `inputs.is_training` or infer from context\n",
        "\n",
        "    # In Keras functional API, you typically chain layers, and Keras handles\n",
        "    # the `training` argument propagation. When defining a custom Layer's `call`\n",
        "    # method with `training` as a parameter, Keras expects it to be passed.\n",
        "\n",
        "    # A more robust way in the functional API is to define `training` as a placeholder:\n",
        "    # `training_arg = tf.keras.backend.learning_phase()`\n",
        "    # And then pass `training=training_arg` to your custom layers.\n",
        "    # However, for simple Dropout control within custom layers, Keras usually handles this automatically\n",
        "    # if the `call` signature matches `(self, inputs, training=None)`.\n",
        "    # Since your `call` does NOT have a default value for `training`, it's a required argument.\n",
        "\n",
        "    # Let's directly pass it to the TransformerBlock.\n",
        "    # When building the model, we implicitly define the computation graph.\n",
        "    # Keras will handle `training` appropriately during .fit() and .predict().\n",
        "    # The common practice is to pass it down.\n",
        "    # x = transformer_block(x, training=inputs.is_training) # This is how it's often handled implicitly.\n",
        "    # But since it's a direct Layer call, let's ensure it gets the argument.\n",
        "\n",
        "    # The most common way to handle this in Keras Functional API\n",
        "    # when a custom layer explicitly requires `training` is to ensure\n",
        "    # your `call` method in `TransformerBlock` looks like:\n",
        "    # `def call(self, inputs, training=None):`\n",
        "    # (with a default `None`).\n",
        "    # If you remove the default, then Keras doesn't know how to fill it automatically\n",
        "    # in the functional API graph building unless you explicitly provide it in `Input`.\n",
        "\n",
        "    # Let's fix your `TransformerBlock` call method first, as it's the more Keras-idiomatic way.\n",
        "    # I will provide the updated TransformerBlock first.\n",
        "\n",
        "    # Let's assume you have updated TransformerBlock's call method:\n",
        "    # `def call(self, inputs, training=None):`\n",
        "    # If so, then `x = transformer_block(x)` should work.\n",
        "    # If not, you *must* pass it.\n",
        "\n",
        "    # Given your current error, `training` is *missing*, meaning no default.\n",
        "    # The most direct fix is to ensure the functional API correctly provides it.\n",
        "    # Keras usually provides it automatically if the signature is `call(self, inputs, training=None)`\n",
        "\n",
        "    # Temporary explicit fix:\n",
        "    # x = transformer_block(x, training=True) # This forces training mode, not ideal for inference\n",
        "    # Better: modify TransformerBlock's call signature.\n",
        "\n",
        "    # Let's assume you've used the recommended signature `def call(self, inputs, training=None):`\n",
        "    # then the following line is correct as is:\n",
        "    x = transformer_block(x) # Call the block without explicitly passing 'training' if it has a default\n",
        "\n",
        "    # Flatten the output from the TransformerBlock so that the Dense layer can operate on a single vector per sample.\n",
        "    x = layers.Flatten()(x) # Shape: (None, LOOK_BACK * EMBED_DIM)\n",
        "\n",
        "    # The Dense layer now outputs a flat vector of `predict_ahead * num_features`\n",
        "    outputs = layers.Dense(output_sequence_length * output_features_count)(x) # Shape: (None, PREDICT_AHEAD * NUM_FEATURES)\n",
        "\n",
        "    # Reshape to the desired output sequence shape\n",
        "    outputs = layers.Reshape((output_sequence_length, output_features_count))(outputs) # Shape: (None, PREDICT_AHEAD, NUM_FEATURES)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_transformer_model(input_shape, output_sequence_length, output_features_count):\n",
        "    \"\"\"\n",
        "    Builds the Transformer model for time series prediction.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input sequences (look_back, num_features).\n",
        "        output_sequence_length (int): The length of the output sequence (predict_ahead).\n",
        "        output_features_count (int): The number of features in the output.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: The compiled Transformer model.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape) # (None, LOOK_BACK, NUM_FEATURES)\n",
        "\n",
        "    # Project input features to EMBED_DIM before positional embedding\n",
        "    x = layers.TimeDistributed(layers.Dense(EMBED_DIM))(inputs) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
        "\n",
        "    # Positional Embedding for input sequence\n",
        "    x = PositionalEmbedding(input_shape[0], EMBED_DIM)(x) # Shape: (None, LOOK_BACK, EMBED_DIM)\n",
        "\n",
        "    # Apply Transformer blocks\n",
        "    transformer_block = TransformerBlock(EMBED_DIM, NUM_HEADS, FF_DIM, DROPOUT_RATE)\n",
        "    # Pass the `training` argument, which Keras automatically provides when\n",
        "    # defining a model via the functional API.\n",
        "    x = transformer_block(x, training=True) # <--- FIX: Pass `training=True` here, or better, pass `inputs.is_training` or infer from context\n",
        "\n",
        "    # In Keras functional API, you typically chain layers, and Keras handles\n",
        "    # the `training` argument propagation. When defining a custom Layer's `call`\n",
        "    # method with `training` as a parameter, Keras expects it to be passed.\n",
        "\n",
        "    # A more robust way in the functional API is to define `training` as a placeholder:\n",
        "    # `training_arg = tf.keras.backend.learning_phase()`\n",
        "    # And then pass `training=training_arg` to your custom layers.\n",
        "    # However, for simple Dropout control within custom layers, Keras usually handles this automatically\n",
        "    # if the `call` signature matches `(self, inputs, training=None)`.\n",
        "    # Since your `call` does NOT have a default value for `training`, it's a required argument.\n",
        "\n",
        "    # Let's directly pass it to the TransformerBlock.\n",
        "    # When building the model, we implicitly define the computation graph.\n",
        "    # Keras will handle `training` appropriately during .fit() and .predict().\n",
        "    # The common practice is to pass it down.\n",
        "    # x = transformer_block(x, training=inputs.is_training) # This is how it's often handled implicitly.\n",
        "    # But since it's a direct Layer call, let's ensure it gets the argument.\n",
        "\n",
        "    # The most common way to handle this in Keras Functional API\n",
        "    # when a custom layer explicitly requires `training` is to ensure\n",
        "    # your `call` method in `TransformerBlock` looks like:\n",
        "    # `def call(self, inputs, training=None):`\n",
        "    # (with a default `None`).\n",
        "    # If you remove the default, then Keras doesn't know how to fill it automatically\n",
        "    # in the functional API graph building unless you explicitly provide it in `Input`.\n",
        "\n",
        "    # Let's fix your `TransformerBlock` call method first, as it's the more Keras-idiomatic way.\n",
        "    # I will provide the updated TransformerBlock first.\n",
        "\n",
        "    # Let's assume you have updated TransformerBlock's call method:\n",
        "    # `def call(self, inputs, training=None):`\n",
        "    # If so, then `x = transformer_block(x)` should work.\n",
        "    # If not, you *must* pass it.\n",
        "\n",
        "    # Given your current error, `training` is *missing*, meaning no default.\n",
        "    # The most direct fix is to ensure the functional API correctly provides it.\n",
        "    # Keras usually provides it automatically if the signature is `call(self, inputs, training=None)`\n",
        "\n",
        "    # Temporary explicit fix:\n",
        "    # x = transformer_block(x, training=True) # This forces training mode, not ideal for inference\n",
        "    # Better: modify TransformerBlock's call signature.\n",
        "\n",
        "    # Let's assume you've used the recommended signature `def call(self, inputs, training=None):`\n",
        "    # then the following line is correct as is:\n",
        "    x = transformer_block(x) # Call the block without explicitly passing 'training' if it has a default\n",
        "\n",
        "    # Flatten the output from the TransformerBlock so that the Dense layer can operate on a single vector per sample.\n",
        "    x = layers.Flatten()(x) # Shape: (None, LOOK_BACK * EMBED_DIM)\n",
        "\n",
        "    # The Dense layer now outputs a flat vector of `predict_ahead * num_features`\n",
        "    outputs = layers.Dense(output_sequence_length * output_features_count)(x) # Shape: (None, PREDICT_AHEAD * NUM_FEATURES)\n",
        "\n",
        "    # Reshape to the desired output sequence shape\n",
        "    outputs = layers.Reshape((output_sequence_length, output_features_count))(outputs) # Shape: (None, PREDICT_AHEAD, NUM_FEATURES)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Training Function ---\n",
        "def train_transformer_ts(X_scaled, Y_scaled, input_features_count, output_features_count, look_back, predict_ahead, epochs=50, batch_size=32):\n",
        "    \"\"\"\n",
        "    Trains the Transformer time series prediction model.\n",
        "\n",
        "    Args:\n",
        "        X_scaled (numpy.ndarray): Scaled input sequences.\n",
        "        Y_scaled (numpy.ndarray): Scaled output sequences.\n",
        "        input_features_count (int): Number of features in input.\n",
        "        output_features_count (int): Number of features in output.\n",
        "        look_back (int): Look-back window size.\n",
        "        predict_ahead (int): Number of future steps to predict.\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size for training.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: The trained Transformer model.\n",
        "    \"\"\"\n",
        "    # Build the model\n",
        "    model = build_transformer_model(\n",
        "        input_shape=(look_back, input_features_count),\n",
        "        output_sequence_length=predict_ahead,\n",
        "        output_features_count=output_features_count\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Training Transformer model for {epochs} epochs...\")\n",
        "    model.fit(X_scaled, Y_scaled, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    print(\"Training complete.\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eW51h5UNHQX2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from data/AAPL_market_data.csv...\n",
            "Initial DataFrame shape: (3776, 9)\n",
            "Training Transformer model for 1 epochs...\n",
            "Training complete.\n",
            "\n",
            "Predicted Future Values (original scale) for next 30 steps across 9 features:\n",
            "\n",
            "DataFrame successfully saved to 'data/AAPL_predicted_data.csv'\n"
          ]
        }
      ],
      "source": [
        "# --- 6. Prediction Function ---\n",
        "def predict_future_values(model, last_input_sequence_scaled, scaler_output):\n",
        "    \"\"\"\n",
        "    Predicts future values using the trained Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model (keras.Model): The trained Transformer model.\n",
        "        last_input_sequence_scaled (numpy.ndarray): The last sequence of input data, scaled.\n",
        "                                                    Shape: (1, look_back, num_features).\n",
        "        scaler_output (sklearn.preprocessing.MinMaxScaler): The scaler used for output features.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The inverse-transformed predicted future values.\n",
        "                       Shape: (predict_ahead, num_features).\n",
        "    \"\"\"\n",
        "    # Ensure the input sequence has the correct shape for prediction (batch_size, look_back, num_features)\n",
        "    if last_input_sequence_scaled.ndim == 2:\n",
        "        last_input_sequence_scaled = np.expand_dims(last_input_sequence_scaled, axis=0)\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_scaled = model.predict(last_input_sequence_scaled, verbose=0)\n",
        "\n",
        "    # Reshape the predicted output to 2D for inverse transformation\n",
        "    # The model outputs (batch_size, predict_ahead, num_features)\n",
        "    # We need (predict_ahead * num_features) for the scaler, then reshape back\n",
        "    predicted_scaled_2d = predicted_scaled.reshape(-1, predicted_scaled.shape[-1])\n",
        "\n",
        "    # Inverse transform the prediction\n",
        "    predicted_original = scaler_output.inverse_transform(predicted_scaled_2d)\n",
        "\n",
        "    # Reshape back to (predict_ahead, num_features)\n",
        "    predicted_original = predicted_original.reshape(predicted_scaled.shape[1], predicted_scaled.shape[2])\n",
        "\n",
        "    return predicted_original\n",
        "\n",
        "\n",
        "# --- Example Usage (using the uploaded CSV content) ---\n",
        "\n",
        "# This part demonstrates how to use the functions defined above\n",
        "# In a real application, X_scaled, Y_scaled, scaler_input, scaler_output would be\n",
        "# provided as function arguments based on external data.\n",
        "\n",
        "# Placeholder for the actual content of 'AAPL_merged_combined_data_2025-07-11.csv'\n",
        "# In a real scenario, this would be read from the file directly.\n",
        "# For demonstration, I'll use a simplified string based on the fetched content.\n",
        "# IMPORTANT: Replace this with the actual content fetched from the file for full functionality.\n",
        "ticker = \"AAPL\"\n",
        "input_data = \"data/\" + ticker + \"_market_data.csv\"\n",
        "predicted_data = \"data/\" + ticker + \"_predicted_data.csv\"\n",
        "\n",
        "# 1. Load and preprocess data\n",
        "scaled_data, scaler_input, scaler_output, features = load_and_preprocess_data(input_data)\n",
        "\n",
        "# Determine the number of features based on the preprocessed data\n",
        "NUM_FEATURES = scaled_data.shape[1]\n",
        "\n",
        "# 2. Create sequences\n",
        "X, Y = create_sequences(scaled_data, LOOK_BACK, PREDICT_AHEAD)\n",
        "\n",
        "# Ensure X and Y are reshaped correctly for the model\n",
        "# X: (samples, look_back, num_features)\n",
        "# Y: (samples, predict_ahead, num_features)\n",
        "\n",
        "# For demonstration, let's take a subset if the data is too large for quick execution\n",
        "# Or ensure LOOK_BACK and PREDICT_AHEAD are reasonable for the given data length.\n",
        "# Assuming enough data exists for at least one sequence.\n",
        "\n",
        "# Get the number of features from the scaled data\n",
        "input_features_count = scaled_data.shape[1]\n",
        "output_features_count = scaled_data.shape[1] # Assuming predicting all features\n",
        "\n",
        "# 3. Train the model\n",
        "# Note: For actual training, you would typically split data into training and validation sets.\n",
        "# For this example, we'll train on all generated sequences.\n",
        "if X.shape[0] > 0: # Check if sequences were created\n",
        "    model = train_transformer_ts(\n",
        "        X_scaled=X,\n",
        "        Y_scaled=Y,\n",
        "        input_features_count=input_features_count,\n",
        "        output_features_count=output_features_count,\n",
        "        look_back=LOOK_BACK,\n",
        "        predict_ahead=PREDICT_AHEAD,\n",
        "        epochs=1,  # Set a small number of epochs for demonstration\n",
        "        batch_size=1\n",
        "    )\n",
        "\n",
        "    # 4. Make a prediction\n",
        "    # Get the last sequence from the original scaled data to predict future values\n",
        "    last_input_sequence_scaled = scaled_data[-LOOK_BACK:]\n",
        "\n",
        "    if last_input_sequence_scaled.shape[0] == LOOK_BACK:\n",
        "        predicted_future_values = predict_future_values(model, last_input_sequence_scaled, scaler_output)\n",
        "        print(\"\\nPredicted Future Values (original scale) for next\", PREDICT_AHEAD, \"steps across\", NUM_FEATURES, \"features:\")\n",
        "\n",
        "        # --- NEW CODE TO ADD DATES ---\n",
        "        # 1. Load the original DataFrame to get the last date\n",
        "        original_df = pd.read_csv(input_data) # Re-read for full dataframe\n",
        "        original_df['date'] = pd.to_datetime(original_df['date'])\n",
        "        last_known_date = original_df['date'].iloc[-1]\n",
        "\n",
        "        # 2. Generate future dates\n",
        "        future_dates = pd.date_range(start=last_known_date + pd.Timedelta(days=1),\n",
        "                                     periods=PREDICT_AHEAD,\n",
        "                                     freq='B') # 'B' for business day frequency\n",
        "\n",
        "        # 3. Create a DataFrame for better readability of predictions with dates\n",
        "        predicted_df = pd.DataFrame(predicted_future_values, columns=features)\n",
        "        predicted_df.insert(0, 'date', future_dates) # Insert 'date' column at the beginning\n",
        "        save_dataframe_to_csv(predicted_df, ticker, predicted_data)\n",
        "    else:\n",
        "        print(\"Not enough data to create the last input sequence for prediction.\")\n",
        "else:\n",
        "    print(\"Not enough data to create sequences for training and prediction. Adjust LOOK_BACK or PREDICT_AHEAD constants or provide more data.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deeplearning-python-3.10_numpy1.26.4-M4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
